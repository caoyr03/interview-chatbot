interview_topic,section,question,answer,,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,What is ‘Overfitting’ in Machine learning?,"In machine learning, when a statistical model describes random error or noise instead of underlying relationship ‘overfitting’ occurs.  When a model is excessively complex, overfitting is normally observed, because of having too many parameters with respect to the number of training data types. The model exhibits poor performance which has been overfit.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,How can you avoid overfitting ?,"By using a lot of data overfitting can be avoided, overfitting happens relatively as you have a small dataset, and you try to learn from it. But if you have a small database and you are forced to come with a model based on that. In such situation, you can use a technique known as cross validation. In this method the dataset splits into two section, testing and training datasets, the testing dataset will only test the model while, in training dataset, the datapoints will come up with the model.In this technique,  a model is usually given a dataset of a known data on which training (training data set) is run and a dataset of unknown data against which the model is tested. The idea of cross validation is to define a dataset to “test” the model in the training phase.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Theory,What are the five popular algorithms of Machine Learning?,"Decision Tree, Neural Network (back propagation), Nearest Neighbor, Support vector machine, Random Forest",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Theory,What are the different Algorithm techniques in Machine Learning?,"Supervised Learning, Unsupervised Learning, Reinforcement Learning, Semi-supervised Learning",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Theory,What are the three stages to build the hypothesis or model in Machine Learning?,"Model Building, Model Testing, Applying the model",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Theory,What is the difference between artificial learning and machine learning?,"Designing and developing algorithms according to the behaviours based on empirical data are known as Machine Learning.  While artificial intelligence in addition to machine learning, it also covers other aspects like knowledge representation, natural language processing, planning, robotics etc.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,What is classifier in machine learning?,"A classifier in a Machine Learning is a system that inputs a vector of discrete or continuous feature values and outputs a single discrete value, the class.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Theory,What is the difference between supervised learning and unsupervised learning?,"Supervised learning requires training labeled data. For example, in order to do classification (a supervised learning task), you’ll need to first label the data you’ll use to train the model to classify data into your labeled groups. Unsupervised learning, in contrast, does not require labeling data explicitly.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Unsupervised Learning,How is KNN different from k-means clustering?,"K-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an unsupervised clustering algorithm. While the mechanisms may seem similar at first, what this really means is that in order for K-Nearest Neighbors to work, you need labeled data you want to classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and gradually learn how to cluster them into groups by computing the mean of the distance between different points.
The critical difference here is that KNN needs labeled points and is thus supervised learning, while k-means doesn’t — and is thus unsupervised learning.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Evaluation,Explain how a ROC curve works.,The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).,,,,,,,,,,,,,,,,,,,,,
Machine Learning,Evaluation,Define precision and recall.,"Recall is also known as the true positive rate: the amount of positives your model claims compared to the actual number of positives there are throughout the data. Precision is also known as the positive predictive value, and it is a measure of the amount of accurate positives your model claims compared to the number of positives it actually claims",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What is Bayes’ Theorem? How is it useful in a machine learning context?,"Bayes’ Theorem gives you the posterior probability of an event given what is known as prior knowledge.
Mathematically, it’s expressed as the true positive rate of a condition sample divided by the sum of the false positive rate of the population and the true positive rate of a condition. ",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,Why is “Naive” Bayes naive?,"The Naive Bayes Algorithm is based on the Bayes Theorem. Bayes’ theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event.
The Algorithm is ‘naive’ because it makes assumptions that may or may not turn out to be correct.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Evaluation,What’s the difference between Type I and Type II error?,"Type I error is a false positive, while Type II error is a false negative. Briefly stated, Type I error means claiming something has happened when it hasn’t, while Type II error means that you claim nothing is happening when in fact something is.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Deep Learning,"What is deep learning, and how does it contrast with other machine learning algorithms?","Deep learning is a subset of machine learning that is concerned with neural networks: how to use backpropagation and certain principles from neuroscience to more accurately model large sets of unlabelled or semi-structured data. In that sense, deep learning represents an unsupervised learning algorithm that learns representations of data through the use of neural nets.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Theory,What’s the difference between a generative and discriminative model?,A generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks.,,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,How is a decision tree pruned?,"Pruning is a technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that provide little power to classify instances. So, when we remove sub-nodes of a decision node, this process is called pruning or opposite process of splitting.Pruning can happen bottom-up and top-down, with approaches such as reduced error pruning and cost complexity pruning.
Reduced error pruning is perhaps the simplest version: replace each node. If it doesn’t decrease predictive accuracy, keep it pruned. While simple, this heuristic actually comes pretty close to an approach that would optimize for maximum accuracy.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Evaluation,What’s the F1 score? How would you use it?,"The F1 score is a measure of a model’s performance. It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst. You would use it in classification tests where true negatives don’t matter much.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,When should you use classification over regression?,"Classification produces discrete values and dataset to strict categories, while regression gives you continuous results that allow you to better distinguish differences between individual points. You would use classification over regression if you wanted your results to reflect the belongingness of data points in your dataset to certain explicit categories (ex: If you wanted to know whether a name was male or female rather than just how correlated they were with male and female names.)",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,How do you ensure you’re not overfitting with a model?,"1- Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data.
2- Use cross-validation techniques such as k-folds cross-validation.
3- Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting.",, the model ,simpler: reduce, variance ,by ,taking into ,account ,fewer ,variables ,and ,"parameters,", thereby ,removing ,some ,of ,the ,noise ,in ,the ,training ,data.
Machine Learning,Evaluation,What evaluation approaches would you work to gauge the effectiveness of a machine learning model?,"You would first split the dataset into training and test sets, or perhaps use cross-validation techniques to further segment the dataset into composite sets of training and test sets within the data. You should then implement a choice selection of performance metrics: here is a fairly comprehensive list. You could use measures such as the F1 score, the accuracy, and the confusion matrix. What’s important here is to demonstrate that you understand the nuances of how a model is measured and how to choose the right performance measures for the right situations.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Theory,What are 3 ways of reducing dimensionality?,"	Removing collinear features.
	Performing PCA, ICA, or other forms of algorithmic dimensionality reduction.
	Combining features with feature engineering.",,cross-valid,ation technique,s such as ,k-f,olds cross-v,alidatio,n.,,,,,,,,,,,,,
Machine Learning,Theory,"If you split your data into train/test splits, is it still possible to overfit your model?","Yes, it's definitely possible. One common beginner mistake is re-tuning a model or training new models with different parameters after seeing its performance on the test set.
In this case, its the model selection process that causes the overfitting. The test set should not be tainted until you're ready to make your final selection.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,What are the advantages and disadvantages of decision trees?,"Advantages: Decision trees are easy to interpret, nonparametric (which means they are robust to outliers), and there are relatively few parameters to tune.
Disadvantages: Decision trees are prone to be overfit. However, this can be addressed by ensemble methods like random forests or boosted trees.",,regularizat,ion techniques ,such as LA,SSO, that penali,ze certa,in mod,el paramet,ers ,if they’re ,likely to, cause ov,erfit,tin,g.,,,,,
Machine Learning,Supervised Learning,What are the advantages and disadvantages of neural networks?,"Advantages: Neural networks (specifically deep NNs) have led to performance breakthroughs for unstructured datasets such as images, audio, and video. Their incredible flexibility allows them to learn patterns that no other ML algorithm can learn.
Disadvantages: However, they require a large amount of training data to converge. It's also difficult to pick the right architecture, and the internal ""hidden"" layers are incomprehensible.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,How can you choose a classifier based on training set size?,"If training set is small, high bias / low variance models (e.g. Naive Bayes) tend to perform better because they are less likely to be overfit.
If training set is large, low bias / high variance models (e.g. Logistic Regression) tend to perform better because they can reflect more complex relationships.",,,,,,,,,,,,,,,,,,,,,
Programming,Data Structure,What are some differences between a linked list and an array?,"An array is an ordered collection of objects. A linked list is a series of objects with pointers that direct how to process them sequentially. An array assumes that every element has the same size, unlike the linked list. A linked list can more easily grow organically: an array has to be pre-defined or re-defined for organic growth. Shuffling a linked list involves changing which points direct where — meanwhile, shuffling an array is more complex and takes more memory.",,,,,,,,,,,,,,,,,,,,,
Programming,Data Structure,Describe a hash table.,A hash table is a data structure that produces an associative array. A key is mapped to certain values through the use of a hash function. They are often used for tasks such as database indexing.,,,,,,,,,,,,,,,,,,,,,
Programming,Python,Name native data structures in Python,"Dictionaries, Lists, Sets, Strings, Tuples",,,,,,,,,,,,,,,,,,,,,
Programming,Python,"Of Python data structures, which are mutable and which are immutable?","Lists, dictionaries, and sets are mutable. This means that you can change their content without changing their identity. Strings and tuples are immutable, as their contents can’t be altered once they’re created.",,,,,,,,,,,,,,,,,,,,,
Programming,Python,"Is There a Way to Get a List of All the Keys in a Dictionary? If So, How Would You Do It?",Mydict.keys(),,,,,,,,,,,,,,,,,,,,,
Programming,Python,Can You Explain What a List or Dict Comprehension Is?,"When you need to create a new list from other iterables, you have to use list comprehensions. As lists comprehensions return list results, they will be made up of brackets that contain the expressions that need to be executed for each element. Along with the loop, these can be iterated over each element.  
Example of the basic syntax:
new_list = [expression for_loop_one_or_more conditions]",,,,,,,,,,,,,,,,,,,,,
Programming,Python,When Would You Use a List vs. a Tuple vs. a Set in Python?,"A list is a common data type that is highly flexible. It can store a sequence of objects that are mutable, so it’s ideal for projects that demand the storage of objects that can be changed later.
A tuple is similar to a list in Python, but the key difference between them is that tuples are immutable. They also use less space than lists and can only be used as a key in a dictionary. Tuples are a perfect choice when you want a list of constants.
Sets are a collection of unique elements that are used in Python. Sets are a good option when you want to avoid duplicate elements in your list. This means that whenever you have two lists with common elements between them, you can leverage sets to eliminate them.",,,,,,,,,,,,,,,,,,,,,
Programming,Python,"What Packages in the Standard Library, Useful for Data Science Work, Do You Know?","NumPy
NumPy (or Numerical Python) is one of the principle packages for data science applications. It’s often used to process large multidimensional arrays, extensive collections of high-level mathematical functions, and matrices. Implementation methods also make it easy to conduct multiple operations with these objects.
There have been many improvements made over the last year that have resolved several bugs and compatibility issues. NumPy is popular because it can be used as a highly efficient multi-dimensional container of generic data. It’s also an excellent library as it makes data analysis simple by processing data faster while using a lot less code than lists.
Pandas
Pandas is a Python library that provides highly flexible and powerful tools and high-level data structures for analysis. Pandas is an excellent tool for data analytics because it can translate highly complex operations with data into just one or two commands.
Pandas comes with a variety of built-in methods for combining, filtering, and grouping data. It also boasts time-series functionality that is closely followed by remarkable speed indicators.
SciPy
SciPy is another outstanding library for scientific computing. It’s based on NumPy and was created to extend its capabilities. Like NumPy, SciPy’s data structure is also a multidimensional array that’s implemented by NumPy.
The SciPy package contains powerful tools that help solve tasks related to integral calculus, linear algebra, probability theory, and much more.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Unsupervised Learning,Explain Latent Dirichlet Allocation (LDA).,"Latent Dirichlet Allocation (LDA) is a common method of topic modeling, or classifying documents by subject matter.
LDA is a generative model that represents documents as a mixture of topics that each have their own probability distribution of possible words.
The ""Dirichlet"" distribution is simply a distribution of distributions. In LDA, documents are distributions of topics that are distributions of words.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Unsupervised Learning,Explain Principle Component Analysis (PCA).,"PCA is a method for transforming features in a dataset by combining them into uncorrelated linear combinations.
These new features, or principal components, sequentially maximize the variance represented (i.e. the first principal component has the most variance, the second principal component has the second most, and so on).
As a result, PCA is useful for dimensionality reduction because you can set an arbitrary variance cutoff.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Ensemble Learning,Explain bagging,"Bagging, or Bootstrap Aggregating, is an ensemble method in which the dataset is first divided into multiple subsets through resampling.
Then, each subset is used to train a model, and the final predictions are made through voting or averaging the component models.
Bagging is performed in parallel.",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What do you understand by the term Normal Distribution?,"Data is usually distributed in different ways with a bias to the left or to the right or it can all be jumbled up.
However, there are chances that data is distributed around a central value without any bias to the left or right and reaches normal distribution in the form of a bell-shaped curve.",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What is correlation and covariance in statistics?,"Correlation: Correlation is considered or described as the best technique for measuring and also for estimating the quantitative relationship between two variables. Correlation measures how strongly two variables are related.
Covariance: In covariance two items vary together and it’s a measure that indicates the extent to which two random variables change in cycle. It is a statistical term; it explains the systematic relation between a pair of random variables, wherein changes in one variable reciprocal by a corresponding change in another variable.",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What is the difference between Point Estimates and Confidence Interval?,"Point Estimation gives us a particular value as an estimate of a population parameter. Method of Moments and Maximum Likelihood estimator methods are used to derive Point Estimators for population parameters.
A confidence interval gives us a range of values which is likely to contain the population parameter. The confidence interval is generally preferred, as it tells us how likely this interval is to contain the population parameter. This likeliness or probability is called Confidence Level or Confidence coefficient and represented by 1 — alpha, where alpha is the level of significance.",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What is the goal of A/B Testing?,"It is a hypothesis testing for a randomized experiment with two variables A and B.
The goal of A/B Testing is to identify any changes to the web page to maximize or increase the outcome of interest. A/B testing is a fantastic method for figuring out the best online promotional and marketing strategies for your business. It can be used to test everything from website copy to sales emails to search ads",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What is p-value?,"When you perform a hypothesis test in statistics, a p-value can help you determine the strength of your results. p-value is a number between 0 and 1. Based on the value it will denote the strength of the results. The claim which is on trial is called the Null Hypothesis.
Low p-value (≤ 0.05) indicates strength against the null hypothesis which means we can reject the null Hypothesis. High p-value (≥ 0.05) indicates strength for the null hypothesis which means we can accept the null Hypothesis p-value of 0.05 indicates the Hypothesis could go either way. ",,,,,,,,,,,,,,,,,,,,,
Statistics,Problem,How can you generate a random number between 1 – 7 with only a die?,"	To get our 7 equal outcomes we have to reduce this 36 to a number divisible by 7. We can thus consider only 35 outcomes and exclude the other one.
	A simple scenario can be to exclude the combination (6,6), i.e., to roll the die again if 6 appears twice",,,,,,,,,,,,,,,,,,,,,
Statistics,Problem,"A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?","From the question, we can exclude the first case of BB. Thus from the remaining 3 possibilities of BG, GB & BB, we have to find the probability of the case with two girls.
Thus, P(Having two girls given one girl)   =    1 / 3",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What do you understand by statistical power of sensitivity and how do you calculate it?,"Sensitivity is commonly used to validate the accuracy of a classifier (Logistic, SVM, Random Forest etc.).
Sensitivity is nothing but “Predicted True events/ Total events”. True events here are the events which were true and model also predicted them as true.
Calculation of seasonality is pretty straightforward.
Seasonality = ( True Positives ) / ( Positives in Actual Dependent Variable )",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What is regularisation? Why is it useful?,Regularisation is the process of adding tuning parameter to a model to induce smoothness in order to prevent overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constant is often the L1(Lasso) or L2(ridge). The model predictions should then minimize the loss function calculated on the regularized training set.,,,,,,,,,,,,,,,,,,,,,
Statistics,Theroy,What Are Confounding Variables?,"In statistics, a confounder is a variable that influences both the dependent variable and independent variable.",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What Are the Types of Biases That Can Occur During Sampling?,"	Selection bias
	Under coverage bias
	Survivorship bias",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What is Survivorship Bias?,It is the logical error of focusing aspects that support surviving some process and casually overlooking those that did not work because of their lack of prominence. This can lead to wrong conclusions in numerous different means.,,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What is selection Bias?,Selection bias occurs when the sample obtained is not representative of the population intended to be analysed.,,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,What is TF/IDF vectorization?,"TF–IDF is short for term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.
The TF–IDF value increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.",,,,,,,,,,,,,,,,,,,,,
Statistics,Theory,Why we generally use Softmax non-linearity function as last operation in-network?,It is because it takes in a vector of real numbers and returns a probability distribution. It should be clear that the output is a probability distribution: each element is non-negative and the sum over all components is 1.,,,,,,,,,,,,,,,,,,,,,
Machine Learning,Evaluation,Can you explain the difference between a Validation Set and a Test Set?,"A Validation set can be considered as a part of the training set as it is used for parameter selection and to avoid overfitting of the model being built.
On the other hand, a Test Set is used for testing or evaluating the performance of a trained machine learning model.
In simple terms, the differences can be summarized as; training set is to fit the parameters i.e. weights and test set is to assess the performance of the model i.e. evaluating the predictive power and generalization.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,Name algorithms of supervised learning,"Support Vector Machines, Regression, Naive Bayes, Decision Trees, K-nearest Neighbor Algorithm and Neural Networks",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Unsupervised Learning,Name algorithms of unsupervised learning,"Clustering, Anomaly Detection, Neural Networks and Latent Variable Models",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,Explain SVM algorithm in detail.,"SVM stands for support vector machine, it is a supervised machine learning algorithm which can be used for both Regression and Classification. If you have n features in your training data set, SVM tries to plot it in n-dimensional space with the value of each feature being the value of a particular coordinate. SVM uses hyperplanes to separate out different classes based on the provided kernel function.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,What is logistic regression?,Logistic Regression often referred to as the logit model is a technique to predict the binary outcome from a linear combination of predictor variables. ,,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,What Are the Drawbacks of the Linear Model?,"	The assumption of linearity of the errors.
	It can’t be used for count outcomes or binary outcomes
	There are overfitting problems that it can’t solve",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Ensemble Learning,Describe boosting,"Boosting is an iterative technique which adjusts the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. Boosting in general decreases the bias error and builds strong predictive models. However, they may over fit on the training data.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Supervised Learning,What is a Random Forest? How does it work?,"Random forest is a versatile machine learning method capable of performing both regression and classification tasks. It is also used for dimensionality reduction, treats missing values, outlier values. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model. In Random Forest, we grow multiple trees as opposed to a single tree. To classify a new object based on attributes, each tree gives a classification. The forest chooses the classification having the most votes(Overall the trees in the forest) and in case of regression, it takes the average of outputs by different trees.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Deep Learning,Describe the structure of Artificial Neural Networks?,"Artificial Neural Networks works on the same principle as a biological Neural Network. It consists of inputs which get processed with weighted sums and Bias, with the help of Activation Functions.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Deep Learning,How Are Weights Initialized in a Network?,"There are two methods here: we can either initialize the weights to zero or assign them randomly.
Initializing all weights to 0: This makes your model similar to a linear model. All the neurons and every layer perform the same operation, giving the same output and making the deep net useless.
Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to 0. It gives better accuracy to the model since every neuron performs different computations. This is the most commonly used method.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Deep Learning,What Is the Cost Function?,"Also referred to as “loss” or “error,” cost function is a measure to evaluate how good your model’s performance is. It’s used to compute the error of the output layer during backpropagation. We push that error backwards through the neural network and use that during the different training functions.",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Deep Learning,What Are Hyperparameters?,"With neural networks, you’re usually working with hyperparameters once the data is formatted correctly. A hyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, etc.).",,,,,,,,,,,,,,,,,,,,,
Machine Learning,Deep Learning,What Will Happen If the Learning Rate Is Set inaccurately (Too Low or Too High)?,"When your learning rate is too low, training of the model will progress very slowly as we are making minimal updates to the weights. It will take many updates before reaching the minimum point.
If the learning rate is set too high, this causes undesirable divergent behaviour to the loss function due to drastic updates in weights. It may fail to converge (model can give a good output) or even diverge (data is too chaotic for the network to train).
",,,,,,,,,,,,,,,,,,,,,
SQL,Concept,What is the difference between SQL and MySQL?,"SQL is a standard language for retrieving and manipulating structured databases. On the contrary, MySQL is a relational database management system, like SQL Server, Oracle or IBM DB2, that is used to manage SQL databases.",,,,,,,,,,,,,,,,,,,,,
SQL,Concept,What are Constraints in SQL?,"Constraints are used to specify the rules concerning data in the table. It can be applied for single or multiple fields in an SQL table during creation of table or after creationg using the ALTER TABLE command. The constraints are:
NOT NULL - Restricts NULL value from being inserted into a column.
CHECK - Verifies that all values in a field satisfy a condition.
DEFAULT - Automatically assigns a default value if no value has been specified for the field.
UNIQUE - Ensures unique values to be inserted into the field.
INDEX - Indexes a field providing faster retrieval of records.
PRIMARY KEY - Uniquely identifies each record in a table.
FOREIGN KEY - Ensures referential integrity for a record in another table.",,,,,,,,,,,,,,,,,,,,,
SQL,Concept,What are some common clauses used with SELECT query in SQL?,"	WHERE clause in SQL is used to filter records that are necessary, based on specific conditions.
	ORDER BY clause in SQL is used to sort the records based on some field(s) in ascending (ASC) or descending order (DESC).
	GROUP BY clause in SQL is used to group records with identical data and can be used in conjuction with some aggregation functions to produce summarized results from the database.
	HAVING clause in SQL is used to filter records in combination with the GROUP BY clause. It is different from WHERE, since WHERE clause cannot filter aggregated records.",,,,,,,,,,,,,,,,,,,,,
SQL,Concept,"What are UNION, MINUS and INTERSECT commands?
","The UNION operator combines and returns the result-set retrieved by two or more SELECT statements.
The MINUS operator in SQL is used to remove duplicates from the result-set obtained by the second SELECT query from the result-set obtained by the first SELECT query and then return the filtered results from the first.
The INTERSECT clause in SQL combines the result-set fetched by the two SELECT statements where records from one match the other and then returns this intersection of result-sets.",,,,,,,,,,,,,,,,,,,,,
SQL,Concept,What is Cursor? How to use a Cursor?,"A database cursor is a control structure that allows for traversal of records in a database. Cursors, in addition, facilitates processing after traversal, such as retrieval, addition and deletion of database records. They can be viewed as a pointer to one row in a set of rows.
	DECLARE a cursor after any variable declaration. The cursor declaration must always be associated with a SELECT Statement.
	Open cursor to initialize the result set. The OPEN statement must be called before fetching rows from the result set.
	FETCH statement to retrieve and move to the next row in the result set.
	Call the CLOSE statement to deactivate the cursor.
	Finally use the DEALLOCATE statement to delete the cursor definition and release the associated resources.",,,,,,,,,,,,,,,,,,,,,
SQL,Concept,List the different types of relationships in SQL.,"	One-to-One - This can be defined as the relationship between two tables where each record in one table is associated with the maximum of one record in the other table.
	One-to-Many & Many-to-One - This is the most commonly used relationship where a record in a table is associated with multiple records in the other table.
	Many-to-Many - This is used in cases when multiple instances on both sides are needed for defining a relationship.
	Self Referencing Relationships - This is used when a table needs to define a relationship with itself.",,,,,,,,,,,,,,,,,,,,,
SQL,Concept,What is Normalization?,"Normalization represents the way of organizing structured data in the database efficiently. It includes creation of tables, establishing relationships between them, and defining rules for those relationships. Inconsistency and redundancy can be kept in check based on these rules, hence, adding flexibility to the database.",,,,,,,,,,,,,,,,,,,,,
SQL,Concept,"What are the TRUNCATE, DELETE and DROP statements?","DELETE statement is used to delete rows from a table.	TRUNCATE command is used to delete all the rows from the table and free the space containing the table.	DROP command is used to remove an object from the database. If you drop a table, all the rows in the table is deleted and the table structure is removed from the database.

",,,,,,,,,,,,,,,,,,,,,
SQL,Concept,What is Collation? What are the different types of Collation Sensitivity?,"	Collation refers to a set of rules that determine how data is sorted and compared. Rules defining the correct character sequence are used to sort the character data. It incorporates options for specifying case-sensitivity, accent marks, kana character types and character width. Below are the different types of collation sensitivity:
	Case sensitivity: A and a are treated differently.
	Accent sensitivity: a and á are treated differently.
	Kana sensitivity: Japanese kana characters Hiragana and Katakana are treated differently.
	Width sensitivity: Same character represented in single-byte (half-width) and double-byte (full-width) are treated differently.",,,,,,,,,,,,,,,,,,,,,
